from pyspark.sql.functions import explode,col,lit,monotonically_increasing_id
from pyspark.sql.types import *
import re
from Csv_Transformer import Transform_csv
from Json_Transformer import Transform_Json
               
def list_files_recursive(base_path):
    files = []
    try:
        items = dbutils.fs.ls(base_path)
        for item in items:
            if item.isDir():
                files.extend(list_files_recursive(item.path))  # Recurse into subdir
            else:
                files.append(item.path)
    except Exception as e:
        print(f"Error accessing {base_path}: {e}")
    return files


catalog = "landing_data"
query=f"show schemas from {catalog}"
schemas_df = spark.sql(query)
schemas = [row["databaseName"] for row in schemas_df.collect()]

for schema in schemas:
    q=f"SHOW VOLUMES IN {catalog}.{schema}"
    volumes_df = spark.sql(q)
    volume_names = [row["volume_name"] for row in volumes_df.collect()]
    
    for volume in volume_names:
        volume_path = f"dbfs:/Volumes/{catalog}/{schema}/{volume}"
        print(f"Scanning: {volume_path}")
        all_files = list_files_recursive(volume_path)
        
        for file_path in all_files:
            print(f"{file_path}")
            if(file_path.endswith(".csv")):
                Transform_csv(file_path,schema,volume)
            elif(file_path.endswith(".json")):
                Transform_Json(file_path,schema,volume)



